{
	"nodes":[
		{"id":"b2a5062260fc0045","x":-780,"y":-40,"width":315,"height":400,"type":"file","file":"PhD Related Items/PHD Stuff/Boston/Interview Research/interviewer profile pictures/marc howard profile.png"},
		{"id":"60e9cb15144f03ca","x":-1500,"y":-40,"width":678,"height":339,"type":"text","text":"### Dr. Marc Howard\nBrain, Behavior, and Cognition Program Director\n\nDirector of Theoretical Cognitive Neuroscience Lab"},
		{"id":"5f59c73b5ed1027a","x":-400,"y":0,"width":12640,"height":6460,"type":"group","label":"Cognitive Computation using neural representations of time and space in the Laplace Domain"},
		{"id":"a363f578a9fe4420","x":4680,"y":260,"width":640,"height":280,"color":"3","type":"text","text":"### $\\tilde{f}(\\tilde{x})$\nBrain's estimate from the actual function of the world over a <mark style=\"background: #FFB86CA6;\">set of neurons</mark>."},
		{"id":"b9a6b3110aafb645","x":4680,"y":560,"width":640,"height":280,"color":"3","type":"text","text":"### $\\tilde{f}(\\tilde{x_0})$\nBrain's estimate from the actual function of the world at a <mark style=\"background: #FF5582A6;\">specific neuron</mark>."},
		{"id":"47bccf04c2a346ab","x":4680,"y":880,"width":640,"height":280,"color":"3","type":"text","text":"### $x_0$\nThe receptor at a physical location."},
		{"id":"9ac3e88b307493d1","x":4680,"y":1180,"width":640,"height":240,"color":"3","type":"text","text":"### $f(x)$\nThe true function out in the world."},
		{"id":"5e180fb6149e5ff4","x":3200,"y":2600,"width":1760,"height":400,"type":"text","text":"## Coding for past events as a function of time in the brain\nComputational neuroscientists have long proposed models with sequentially-activated neurons could represent past events. The observation that memory is less precise for less recent events has led to the proposal that this record of the past is **compressed**, such that the time at which recent events occurred has better resolution than events further in the past. (This is analogous to the compression of the visual system where regions of visual space near the fovea have a much greater resolution than regions further from the fovea)"},
		{"id":"a124437bf125f561","x":3200,"y":3040,"width":1760,"height":760,"type":"text","text":"### Function and Time resolution\n\nAt time $t$ the brain tries to estimate the objective past leading up to the present $f(\\tau)$. In this formulation, $\\tau$ runs form 0 → $\\infty$ with zero corresponding to the moment in the immediate past at time $t$.\n\nAt each moment, we can understand the past as a function over the $\\tau$ axis. This function is estimated by a population of neurons that we write as $\\tilde{f}(\\tilde{\\tau})$.\n\n**Cognitive modeling and theoretical work of representations can be used to construct detailed behavioral models of many memory tasks if the representation of the past is compressed.**\n\nWe first argue that the hippocampal time cells have properties predicted for $\\tilde{f}(\\tilde{\\tau})$ and then review evidence suggesting that neurons in the entorhinal cortex of rodents and monkeys show properties consistent with the laplace transform $$F(s)=\\mathcal{L}f(\\tau)$$\n"},
		{"id":"35c98d241e97d645","x":3200,"y":3840,"width":1760,"height":740,"type":"text","text":"### Time cells in the hippocampus code for a compressed timeline of the recent past\n\nTime cells in the hippocampus behave as if they have <mark style=\"background: #FF5582A6;\">receptive fields organized in time</mark>. As a <mark style=\"background: #FFB86CA6;\">triggering event recedes into the past, the event first enters and then exits the \"time field\" of different time cells</mark> indexed by $\\tilde{\\tau}$.\n\nHippocampal time cells have the computational properties one would expect of a compressed representation of what happened when as a function of past time.\n1) Different external stimuli can trigger distinct sequences of hippocampal time cells (these populations carry information about what stimulus happened in the past)\n2) Hippocampal time cells show decreasing temporal accuracy further in the past.\n\nThe number of cells with receptive fields around a particular value $\\tilde{\\tau_0}$ goes down as $\\tilde{\\tau_0}$ goes up. Moreover, the width of receptive fields go up with $\\tilde{\\tau_0}$."},
		{"id":"d0f83f8a8ddf1e00","x":3200,"y":4640,"width":1760,"height":1080,"type":"text","text":"### Temporal context cells in entorhinal cortex code for the Laplace transform of a compressed timeline of the past\n\nLet us consider how we would identify <mark style=\"background: #D2B3FFA6;\">neurons coding for the Laplace transform</mark>, $F(s)=\\mathcal{L}f(\\tau)$. \n* Cells coding the Laplace transform of a variable $x$ should show receptive fields that fall off like $e^{-sx}$. \n* A set of neurons coding the Laplace transform of past time $\\tau$ should show receptive fields  fields that go like $e^{-s\\tau}$, with many different values of s across different neurons.\nIf we think of the triggering stimulus as a delta function at time t = 0, it enters $f(\\tau)$ at time $t$ at $\\tau = 0$. At time $t$ after the triggering stimulus, the firing rates should change like $e^{-st}$. <mark style=\"background: #FFB86CA6;\">Observing the firing of a neuron with rate constant s, we should see it change shortly after the triggering stimulus, and then relax back to baseline exponentially in the time after the triggering stimulus</mark>.\n* Cells with **high** values of s (corresponding to fast time constants) should relax <mark style=\"background: #ABF7F7A6;\">quickly</mark>;\n* Cells with **small** values of s (corresponding to slow time constants) should relax more <mark style=\"background: #FF5582A6;\">slowly</mark>;\nWe would expect a **continuum** of $s$ values to describe the continuum of $\\tau$ values. If the <mark style=\"background: #BBFABBA6;\">representation is compressed</mark>, we would see more neurons with fast decay rates than with slow decay rates.\n\nRecent evidence shows that cells in the entorhinal cortex contain temporal information, like hippocampal time cells, but with temporal receptive fields that are from the Laplace transform."},
		{"id":"5305b7cbb0eb81e7","x":3200,"y":1580,"width":964,"height":940,"color":"3","type":"text","text":"## What happens if we cannot put a receptor at $x_0$\nIf we cannot directly place a receptor at a particular physical location $x_0$, how can we estimate functions over variables such as time or allocentric position, or location within an abstract conceptual space?\n\nWe hypothesize that as an intermediate step in estimating $\\tilde{f}(\\tilde{x})$ the brain could construct the Laplace transform of $f(x)$ over another population of neurons. It is described notationally as $$F(s)=\\mathcal{L}f(x) $$\nAnalogous to the way in which $\\tilde{f}(\\tilde{x})$ corresponds to the activity of many neurons indexed by their value of $\\tilde{x}$, $F(s)$ is understandable as a particular pattern of activity over a population of neurons, each indexed by a continuous parameter $s$. "},
		{"id":"21427d98811146dc","x":4208,"y":1580,"width":944,"height":940,"color":"3","type":"text","text":"## Inverse Laplace Transform\n\nBecause neurons in $F(s)$ do not have receptive fields centered on a particular value of $x$ , it is not necessarily intuitive to visualize the connection between $f$ and $F$.\n\nWe can construct $\\tilde{f}(\\tilde{x})$ by inverting the transform: $$\\tilde{f}(\\tilde{x})=L_k^{-1}F(s)$$\nHere $L_k^{-1}$ is a feedforward operator that approximates the inverse Laplace transform. Of course this reconstruction has some information lose.\n\nWe also need to notice some blur in the reconstruction!"},
		{"id":"4ab2df8ec0d51b46","x":-380,"y":3240,"width":1084,"height":620,"color":"4","type":"text","text":"### Section 2: Hippocampus and Entorhinal cortex\n\nIn the second section, we describe recent neurophysiological evidence from the hippocampus and entorhinal cortex. <mark style=\"background: #ADCCFFA6;\">The data shows evidence that hippocampal time cells behave as if they are estimating a function over past time</mark>. Moreover neurons in the entorhinal cortex behave as if they were estimating the Laplace transform of this function over past time.\n<mark style=\"background: #D2B3FFA6;\">To the extent one accepts this empirical account, it means that the brain has a transform/inverse pair for functions of time one synapse away in the medial temporal lobe</mark>."},
		{"id":"29c780d366d4de59","x":-380,"y":3900,"width":1084,"height":660,"color":"4","type":"text","text":"### Section 3: Modeling\n\nIn the third section, we will review modeling work describing how to construct transform/inverse pairs to represent functions over not only time, but:\n* Spatial position\n* Other kinematic variables\n* Accumulated evidence for use in decision-making circuits\n\n<mark style=\"background: #FF5582A6;\">We suggest that the reader should seriously consider the idea that the brain might use transform/inverse pairs to perform cognitive computations in many different domains.</mark>"},
		{"id":"7052c4a20315b70a","x":800,"y":1760,"width":988,"height":260,"type":"text","text":"### Cognitive Data-Independent Operations\nHuman cognition has a powerful symbolic capability that allows us to perform may data-independent operations."},
		{"id":"99dbd3ccbd5c6019","x":800,"y":2060,"width":988,"height":1120,"type":"text","text":"### Concrete Example\nAfter focusing on the figure one could close one's eyes and implement a huge number of operations on the contents of memory.\n1) One could choose to imagine Moe Howard's face translated by 5 cm to the left. (Translation)\n2) Decide if the thought bubble in **A** is above or below Moe's tie. (Subtraction)\nOperations like <mark style=\"background: #FF5582A6;\">translation or subtraction</mark> would have obvious benefits in computational problems like <mark style=\"background: #BBFABBA6;\">spatial navigation</mark>, where we have learned a great deal about functional correlates of neurons in the hippocampus and entorhinal cortex.\n\nIf <mark style=\"background: #FFF3A3A6;\">cognitive data of many different types used the same form of neural representation</mark> then i<mark style=\"background: #FF5582A6;\">f we knew how to build data-independent operators in one domain</mark>, <mark style=\"background: #FFB8EBA6;\">the same computational mechanisms could be reused across many domains of cognition</mark>. A complete set of operations would constitute a \"cognitive map\" that could be used for many different types of information."},
		{"id":"94bf80281f6614c8","x":-380,"y":2240,"width":1084,"height":540,"color":"5","type":"text","text":"### What does this paper accomplish?\n\nThis paper reviews recent evidence that suggests a common form of neural representation for many types of information important in cognition. <mark style=\"background: #ADCCFFA6;\">The basic idea is that the firing rate of populations of neurons represent functions out in the world.</mark> Some populations <mark style=\"background: #FFB8EBA6;\">do not represent these functions directly</mark>, but rather <mark style=\"background: #FFB86CA6;\">represents the Laplace transform of function</mark>.\n\nThis paper proceeds in three sections."},
		{"id":"1db6a665ed747c89","x":-380,"y":2800,"width":1084,"height":420,"color":"4","type":"text","text":"### Section 1: Computing with functions in the Laplace Domain\n\nIn this section, we sketch out in non-technical language the ideas behind this hypothesis. This section will explain what it means to say the brain \"represents a function,\" and what it means for the population of neurons to estimate \"the Laplace transform of a function.\""},
		{"id":"84448150afabe5e5","x":1940,"y":2020,"width":1080,"height":1101,"type":"file","file":"PhD Related Items/PHD Stuff/Boston/Interview Research/general images/Pasted image 20230209234253.png"},
		{"id":"2568279d551889ad","x":3200,"y":200,"width":1300,"height":1360,"color":"3","type":"text","text":"# Computing with functions in the Laplace domain\n\nWe argue that the brain, at least in some cases, computes using _functions_ describing information over some continuum (Figure A). \n\nConsider some function $f$ defining a scalar value in the external world over some domain $x$, $f(x)$, for instance, in <mark style=\"background: #FFB86CA6;\">vision the pattern of light in a greyscale image as a function of retinal position</mark>. The activity of neurons along the retina along the retinal surface estimates this function.\n\nTo distinguish the brain's internal estimate from the actual function in the world, we will write $\\tilde{f}(\\tilde{x})$ to describe the activity over a population of neurons. \n\nThe value at a particular location $\\tilde{f}(\\tilde{x_0})$ corresponds to the activation of the receptor that is indexed to the physical location $x_0$. \n\nWe understand $\\tilde{f}(\\tilde{x})$ to mean the activation of all the receptors over all their location. The continuous parameters associated with each neuron, $\\tilde{x}$ maps onto the continuum of $x$, enabling the population to distinguish many different functions $f(x)$.  \n\nWe can understand the particular shape of the receptive fields as basis functions over the domain $x$. We will assume that the number of receptors is very large and the distance between their centers is small so that we can think of $\\tilde{f}(\\tilde{x})$ as if it was a function over a continuous variable. Note that the density of receptors need not be constant in different regions of $x$.\n\n"},
		{"id":"d73ecf2e61146783","x":5000,"y":2600,"width":1480,"height":1942,"type":"file","file":"PhD Related Items/PHD Stuff/Boston/Interview Research/general images/compressed_timeline.png"},
		{"id":"76c3b251fdd5c138","x":5200,"y":1580,"width":794,"height":940,"color":"3","type":"text","text":"## Why Laplace?\n\nIt can efficiently implement data-independent operations on function in the Laplace domain. It many cases, its more computationally efficient to convert to $F(s)$ perform the correct operation to produce the transformation and then take the inverse laplace to get the original equation back."},
		{"id":"8fefa7dd46477fb3","x":6520,"y":2600,"width":1863,"height":820,"type":"text","text":"## Time and memory outside the MTL\n\nThe <mark style=\"background: #BBFABBA6;\">entorhinal cortex and hippocampus</mark> are believed to be <mark style=\"background: #BBFABBA6;\">important in episodic memory</mark>. Computational modeling suggests that a representation like $\\tilde{f}(\\tilde{\\tau})$ is also useful for other \"kinds\" of memory, including short-term working memory tasks, conditioning tasks, as well as interval timing tasks.\n\n^ This above knowledge suggests that other brain regions have <mark style=\"background: #FF5582A6;\">access to representations</mark> like $\\tilde{f}(\\tilde{\\tau})$. \nTime cells with more or less the same properties of hippocampal time cells have been observed in the:\n* <mark style=\"background: #BBFABBA6;\">Striatum</mark>\n* <mark style=\"background: #FFB8EBA6;\">Medial Prefrontal Cortex</mark>\n* <mark style=\"background: #D2B3FFA6;\">Lateral Prefrontal Cortex</mark>\n* <mark style=\"background: #ABF7F7A6;\">Dorsolateral Prefrontal Cortex</mark>\n\nThe fact that this kind of <mark style=\"background: #FFB86CA6;\">representation is widespread suggests that many different types of memory utilize a compressed timeline of the past</mark>. Ramping neurons observed outside of the Entorhinal Cortex (EH) could also be manifestations of the Laplace transform of the past, but this hypothesis has not thus far been explicitly tested."},
		{"id":"bb8f3afaa4b720f5","x":7880,"y":3460,"width":649,"height":76,"type":"text","text":"Monotonically - Always decreasing"},
		{"id":"42f5f610dc6e711b","x":800,"y":1180,"width":988,"height":220,"type":"text","text":"### Time Cells (Hippocampus)\nCan be understood as a <mark style=\"background: #FF5582A6;\">compressed estimate of events as a function of the past</mark>."},
		{"id":"d93c3cf0e8ed3120","x":1940,"y":1180,"width":763,"height":720,"type":"text","text":"Other functional cell types in the hippocampus/related regions, including border cells, place cells, trajectory coding, splitter cells, can be understood as <mark style=\"background: #ADCCFFA6;\">coding for function over space or past movements or their Laplace transforms</mark>.\n\nMore abstract quantities, like distance in an abstract conceptual space or numerosity could also be mapped onto populations of neurons coding for the Laplace transform of functions over those variables."},
		{"id":"2dcb6b3f0f36712c","x":800,"y":1450,"width":988,"height":270,"type":"text","text":"### Temporal Context Cells (Entorhinal Cortex)\nCan be understood as the <mark style=\"background: #FFF3A3A6;\">Laplace transform of the function in the time cells</mark>."},
		{"id":"20f934262e924325","x":10560,"y":5660,"width":1660,"height":780,"type":"text","text":"### Building a Laplace-Domain Model\n\nWith this understanding, it is straightforward to build a Laplace-domain model of the diffusion model by constructing two populations, one of which estimates the distance to $X_t$ to the lower bound and one that estimates the distance to the upper bound.\n\nWe will subscript the two populations such that $F_R(s)$ and $F_R(s)$ correspond to the Laplace transforms of these two function and $\\tilde{f}_R(\\tilde{X})$ and $\\tilde{f}_L(\\tilde{X})$ correspond to the inverse transforms. In the diffusion model evidence for one alternative reduces the evidence for the other alternative so $\\alpha_L(t)=-\\alpha_R(t)$. \n\nA decision is made when the \"particle\" reaches the smallest value of $\\tilde{x}$ in one of the populations. Setting $\\alpha_L$ and $\\alpha_R$ to be non-zero and have the same sign effectively changes the decision bounds. Positive paired values of $\\alpha$ have the effect of widening the decision bounds.\n\n"},
		{"id":"96ff77bbeb96d32c","x":8560,"y":960,"width":1863,"height":1620,"color":"2","type":"text","text":"## Compressed function of other variables\n\nA general framework for cognitive computation in the brain requires that representations of many different variables use the same \"neural currency.\" <mark style=\"background: #D2B3FFA6;\">The same formalism utilizing the Laplace transform and its inverse can give rise not only to functions over time but function over many other variables as well</mark>. \n\nThe basic idea is the equations implementing the Laplace transform of a function of time are modulated by the **rate of change of some variables** $x$. We refer to the modulation factor at time $t$ as $\\alpha(t)$. At the cellular level, $\\alpha(t)$ is <mark style=\"background: #FFB86CA6;\">understandable as a gain factor that changes the slope of the f-i curve relating to firing rate to input current</mark>.\n\nIf all of the neurons participating in the transform are modulated at each moment by $\\alpha(t) = dx/dt$, then $F(s)$ holds the transform with respect to $x$ rather than time, $F(s)=\\mathcal{L}f(x)$. When one inverts the transform, with $L_k^{-1}$, this results in an estimate of the function of $x$, $\\tilde{f}(\\tilde{x})=L_k^{-1}F(s)$.\n\nThis strategy can be used to describe different kinds of functions by coding for different input stimuli - different \"what\" information - and choosing $\\alpha(t)$ to be the rate of change of different variables.\n\nIn this section, we discuss computational work representing compressed functions of variables other than time. For instance we will see that this approach <mark style=\"background: #FFB8EBA6;\">can be used to compute functions coding for the relative spatial position of the wall of an enclosure, for past movements as a function of their position in the sequence or the amount of evidence accumulated for one of two alternatives in a simple decision-making task</mark>.\n\nThe first subsection, entitled \"Spatiotemporal trajectories in the medial temporal lobe,\" reviews evidence that transform/inverse pairs of representations can account for a \"particle zoo\" of functional cell types in the MTL during spatiotemporal navigation.\n\nIn the next subsection, entitled \"Accumulated evidence and decision-making\" we describe a neural implementation of widely-used cognitive models for evidence accumulation models using transform/inverse pairs.\n\nFinally, in the last subsection, entitled \"Cognitive models built entirely of transform/inverse pairs,\" we consider the possibility of cognitive models made entirely of transform/inverse pairs and how they could exploit computational properties of the Laplace domain for symbolic computation."},
		{"id":"6190a73abfe94188","x":8560,"y":2620,"width":1863,"height":740,"color":"2","type":"text","text":"## Spatiotemporal trajectories in the medial temporal lobe\n\nIt has long been suggested that the hippocampal place code is a special case of more general form of representation coding for spatial, temporal and other more abstract relationships between events. <mark style=\"background: #ADCCFFA6;\">A wide diversity of functional cell types that communicate information about kinematic variables have been reported in the hippocampus and related structures, including place cels, border cells, splitter cells, trajectory coding cells, speed cells, head direction cells and many more.</mark> \n\nMany of these functional cell types (the most notable exception being grid cells) can be understood as the Laplace transform of a function coding a spatiotemporal trajectory; others can be understood as an approximate inverse of a function.\n\nMoreover, these populations seem to come in pairs, with populations with properties like the Laplace transform in the entorhinal cortex and the populations with properties like the inverse in the hippocampus."},
		{"id":"8c7e315eb81d98d1","x":8560,"y":3380,"width":1863,"height":1040,"color":"2","type":"text","text":"### Border Cells\n\nConsider border cells in the medial entorhinal cortex (MEC). Border cells fire maximally at a location close to the boundary of an environment with a particular orientation. <mark style=\"background: #FF5582A6;\">Border cells fire maximally at a location close to the boundary of an environment with a particular orientation</mark>. \n\n<mark style=\"background: #ABF7F7A6;\">Their firing rate decays monotonically with distance to the boundary.</mark>  We saw earlier that temporal context cells in the entorhinal cortex are perturbed by a specific stimulus and then relax monotonically towards their baseline firing rate over time.\n\nTemporal context cells code the Laplace transform of a function over time $F(s)=\\mathcal{L}f(\\tau)$. The Laplace transform of distance to the boundary $F(s)=\\mathcal{L}f(x)$ would behave similarly, with exponentially-decaying receptive fields in space.\n\n<mark style=\"background: #FFB8EBA6;\">As the animal moves away from a cell's preferred boundary, firing rate would decrease exponentially with distance; as the animals moves towards the boundary the firing rate would increase along the same curve describing the receptive field.</mark> This is possible because $\\alpha(t)$ is the **signed velocity** in the direction of the boundary. If a population of border cells encodes the Laplace transform of distance to the boundaries, then across neurons there should be a wide variety of spatial receptive field sizes, and more neurons should have narrow spatial receptive fields than wide spatial receptive fields.\n\nThe continuum of spatial locations should be mapped onto a continuum of values of $s$ in the population of border cells.\n\n"},
		{"id":"e09662a98987c3d0","x":8560,"y":4480,"width":1863,"height":1640,"color":"2","type":"text","text":"## Boundary Vector Cells\nBy analogy to time cells, which have receptive fields in a circumscribed region of time since a triggering event, the inverse of border cells should generate a population of neurons with circumscribed receptive fields in space. \n\n<mark style=\"background: #FFB8EBA6;\">Boundary Vector cells</mark> (BVCs), observed within the subiculum, have just this property, with elongated firing fields that align with boundaries of an enclosure. If BVSc and place cells are the result of an approximate inverse transform, they should have properties analogous to those observed for populations of time cells. BVCs should have more fields close to boundaries and the width of fields should increase with distance to the boundary.\n\nThis framework organizes other \"cell types\" in the MTL as well. <mark style=\"background: #ABF7F7A6;\">Consider a population of cells coding for the sequence of movements leading up to the present position as a function of distance traveled</mark>. In words, this population codes for a function $f$ that carries information like \"I got here by traveling North for 2 cm; before that I moved West for 10cm …\" In this case, the \"what information\" in the population would be head direction.\n\nIn order to convey this information as a function of traveled distance, we would set $\\alpha(t)$ to be speed. Cells coding the Laplace transform of this kind of function would behave as \"<mark style=\"background: #FF5582A6;\">trajectory-dependent</mark>\" or \"<mark style=\"background: #FF5582A6;\">retrospective coding</mark>\" cells.\n\nCells coding for the inverse transform would manifest as \"splitter\" cells that fire differentially on the central arm of a figure-8 maze during an alternation task depending on the past locations that have been observed in the entorhinal cortex and hippocampus.\n\nOther functional cell types in the MEC can be understood as coding for spatiotemporal trajectories in the Laplace domain or approximating functions.\n* When an animal pauses during a virtual navigation task, a population of MEC cells fire sequentially recording the amount of time since the animal ceased moving.\n* Even speed cells which are believed to map the animal's instantaneous speed onto their firing rate, actually **filter speed as function of time with a spectrum of time constants**.\n\nThis is consistent with the idea that speed cells in MEC are actually coding the Laplace Transform of the history of speed in the time leading up to the present. Te characteristic predictions from this theoretical approach are best evaluated at the level of population and manifest largely as distributions of parameters.\n  "},
		{"id":"635819e439e07be2","x":10560,"y":3900,"width":1660,"height":1720,"type":"text","text":"## Accumulated Evidence and Decision-making\n\nIn many simple decision-making experiments, noisy instantaneous evidence must be integrated over time in order to reach a confident decision. Decades of work in mathematical psychology has resulted in sophisticated computation models for simple evidence accumulation tasks.\n\nThe best known is the diffusion model!\n![[Pasted image 20230212212923.png|800]]\n\nAt each moment during the decision, the observer samples some evidence. The \"particle's position\" at any moment, $X_t$, describes the accumulated evidence for each alternative up to that point. This abstract model aligns to a strategy in which the starting position (usually referred to as $z$) is controlled by the decision-maker's prior expectations and the boundary separation (usually referred to as $a$) describes the degree of confidence the decision-maker requires before making a choice. We can understand the evidence at any moment $t$ as a function $f(x)$ with a peak at a single value $X_t$. The time derivative of the position of the particle is just the instantaneous evidence sampled at time $t$.\n\nWith this understanding, it is straightforward to build a Laplace-domain model of the diffusion model by constructing two populations, one of which estimates the distance of $X_t$ to the lower bound and one that estimates the distance to the upper bound.\n\nWe can understand the evidence at any moment $t$ as a function $f(x)$ with a peak at a single value $X_t$. The time derivative of the position of the particle is just the instantaneous evidence samples at time $t$."},
		{"id":"86bc3bdfcb7fe915","x":10460,"y":960,"width":766,"height":223,"color":"2","type":"text","text":"## $\\alpha(t)$\nA gain factor that changes the slope of the firing rate to input current curve."},
		{"id":"6f3dd0faf5f6f93c","x":10460,"y":1220,"width":1260,"height":1150,"color":"2","type":"file","file":"PhD Related Items/PHD Stuff/Boston/Interview Research/general images/Pasted image 20230212163639.png"},
		{"id":"5cc3a31c6417934d","x":-380,"y":320,"width":1409,"height":240,"color":"2","type":"text","text":"# Cognitive Computation using neural representations of time and space in the Laplace Domain"},
		{"id":"f056f044812e6863","x":-400,"y":1080,"width":3440,"height":3500,"type":"group","label":"Introduction"},
		{"id":"d71f31f1a5eed51b","x":-380,"y":1180,"width":1084,"height":1020,"type":"text","text":"### Abstract\nMemory for the past makes use of a record of what happened when - <mark style=\"background: #FF5582A6;\">a function over past time</mark>.\n\nThere are two cells of concern in this paper:\n\t1) <mark style=\"background: #ADCCFFA6;\">Time Cells</mark> (located in the hippocampus)\n\t2) <mark style=\"background: #D2B3FFA6;\">Temporal Context Cells</mark> (located in the entorhinal cortex)\nThese **both** <mark style=\"background: #FFB86CA6;\">code for events as a function of past time</mark> but with very <mark style=\"background: #FFF3A3A6;\">different receptive fields</mark>.\n\nQuantitative cognitive models of memory and evidence accumulation can also be specified in this framework allowing constraints from both behavior and neurophysiology.\n\n**More generally**, the computational power of the Laplace domain could be important for efficiently implementing data-indpendent operators, which could serve as a basis for neural models of a very broad range of cognitive computations."},
		{"id":"126d9f078cf4a837","x":-880,"y":8834,"width":880,"height":286,"type":"text","text":"### What study?\n\nParticipants two tasks, a well-established relative judgment of recency (JOR) and a new task, judgment of imminence (JOI)."},
		{"id":"e211cea4fca9b4b3","x":-1420,"y":9421,"width":879,"height":1379,"type":"text","text":"### Judgment of Recency\nJOR participants are presented with a sequence of stimuli followed by a probe consisting of two letters from the sequence. The participants have to select the latter that was presented more recently. \n![[Pasted image 20230213025855.png|800]]\n\nThe study confirmed the classic finding that in JOR response time (RT) in correct trials depends on the lag to the more recent probe but not on the lag to the less recent probe.\n\nFound that RT scales sublinearly with the lag of the more recent probe. This is consistent with the hypothesis that memory is organized as a compressed neural timeline."},
		{"id":"76f0b78c0b018321","x":-340,"y":9421,"width":879,"height":1179,"type":"text","text":"### Judgment of Imminence\n\nDesigned as a future-time analog of JOR. By analogy to the way the JOR evaluates participants' ability to judge the relative time at which past events occurred, this new paradigm **tested participants' ability to judge the imminence of future events over a scale of a few seconds**.\n\nThe relative JOR task requires the participant to select the probe item from the previous list that was presented closer to the present. **In contrast**, the JOI paradigm asks participants to select the future probe item that is anticipated closer to the present.\n![[Pasted image 20230213030436.png|800]]\n\n"},
		{"id":"5db5a040ac57bc99","x":680,"y":7431,"width":879,"height":309,"type":"text","text":"# A Computational Model for simulating the future using a memory timeline"},
		{"id":"299eb872b1d7a1ee","x":680,"y":7760,"width":879,"height":120,"type":"text","text":"Keywords: Associative Memory, Timeline, Scale-invariance, Prediction, Sequence Learning"},
		{"id":"aefcde27384feab9","x":680,"y":7940,"width":879,"height":1360,"type":"text","text":"# Abstract\n* The ability to learn temporal relationships and use that knowledge to simulate future events is among the most remarkable aspects of cognition.\n\n# What does this paper accomplish?\nWe evaluate a computational hypothesis that proposes using <mark style=\"background: #FF5582A6;\">associative memory to learn temporal relationships and construct an estimate of the future</mark>.\n\nTwo major Assumptions:\n1) Memory of the recent past is maintained as a compressed neural timeline. **The more recent past is represented with more neurons**.\n2) Input stimuli are associated with the memory timeline through Hebbian learning.\n\n## How do we evaluate this?\nFocus on the data published in a recent behavioral study that <mark style=\"background: #FFB8EBA6;\">examined the similarity between memory and prediction</mark>."},
		{"id":"addc194e183a299d","x":1599,"y":8933,"width":627,"height":207,"type":"text","text":"The temporal history of a stimulus contains information about which stimuli preceded that stimulus and when."},
		{"id":"d00aabd0902c45b2","x":1599,"y":9172,"width":627,"height":128,"type":"text","text":"The average history is then used compute the average future."},
		{"id":"65af1bc9abe7c765","x":1599,"y":8740,"width":627,"height":160,"type":"text","text":"The Associations store the average temporal history for every stimulus."},
		{"id":"31da639e525f5ad4","x":1600,"y":8720,"width":667,"height":600,"type":"group","label":"Process of Assumption 2"},
		{"id":"c3c1243bb6b8c6b8","x":680,"y":9421,"width":1444,"height":339,"type":"text","text":"## Description of the model\n\nModel the JOR and JOI using the same computational framework.\n1) For JOR we construct a logarithmically compressed memory timeline\n2) Then for JOI we use that timeline to form associations between past and present"},
		{"id":"164f2f61e0d17f2d","x":680,"y":9800,"width":1444,"height":2060,"type":"text","text":"### Task 1: Compressed memory timeline\nWe encode each stimulus as a one-hot vector $f$. (For a list of 7 letters, the length of $f$ is equal to 7.) Each element in $f$ is then fed into a two-layer recurrent network with analytically computed weights.\n\nThe first layer is recurrent and for the i-th element of the input, vector $f$ has the following dynamics: $$\\frac{dF_s^{(i)}(t)}{dt} = -sF_s^{(i)}(t)+f^{(i)}(t)$$\nHere $s$ is an N long vector, where N is the number of neurons in $F_s^{(i)}$. The impulse response of the neurons in the recurrent layer decays exponentially with the rate constants $s$.\n![[Pasted image 20230213031602.png|800]]\n\nThis layer encodes an approximation of the Laplace transform. To obtain a timeline that estimates $f^{(i)}(t'<t)$ we invert the Laplace transform using the Post approximation: $$\\tilde{f}_{\\tilde{\\tau}}^{(i)}(t)=\\frac{(-1)^k}{k!}s^{k+1}\\frac{d^k}{ds^k}F_s^{(i)}(t)$$\nwhere $\\tilde{\\tau}=k/s$ is a vector of logarithmically spaced values. These results in a bell-shaped impulse response that activates sequentially across units in $\\tilde{f}_{\\tilde{\\tau}}^{(i)}$ with peaks at $\\tilde{\\tau}$.\n![[Pasted image 20230213032055.png|800]]\n\nFor a sample trial of JOR task, the activity of the nodes in $f$ and $\\tilde{f}$ are shown below.\n\n![[Pasted image 20230213032205.png|900]]"},
		{"id":"66ab7d4ce3125928","x":2160,"y":9800,"width":1203,"height":1340,"type":"text","text":"### Associative memory\n\nTo create the associative memory we use the approach described in (Shankar & Howard, 2021; Tiganj, Cruzado, & Howard, 2019). At each time $t$, associative memory tensor $M$ is update with the outer product of the current input of the current input state $f$ and $\\tilde{f}$. Hence $M$ is a three-tensor. At each moment, $M$ is update with the simple Hebbian learning rule: $$M_{\\tilde{\\tau}}(t) = M_{\\tilde{\\tau}}(t-1)+ f(t)\\tilde{f}_{\\tilde{\\tau}}(t)$$\n* $f$ is a column vector, length N\n* $\\tilde{f}_{\\tilde{\\tau}}$ is a row vector, length N\n* $M_{\\tilde{\\tau}}$ is N by N matrix\n\nFormation of the associative memory is illustrated in the attached image, where the thickness of the green arrows represents the strength of the associations stored in $M$.\n![[Pasted image 20230213170757.png|800]]\n\n\n"},
		{"id":"5f504d3c589125b0","x":2160,"y":11159,"width":1203,"height":1161,"type":"text","text":"### Compressed timeline of the future\n\nThe associative memory is used to construct prediction of the future. To achieve this, we followed the approach described in Tiganj, Cruzado, and Howard (2019). $M$ stores the pairwise temporal relationships between all stimuli subject to logarithmic compression.\n\nFor instance, if we had two stimuli A and B presented with spacing $\\Delta$, $M$ will store that temporal relationship. Note that the knowledge that $A$ was presented $\\Delta$ time before $B$ implies that $B$ was presented $\\Delta$ time after $A$. In other words, $M$ stores the average history, but we can use that information to compute the average future. Specifically, multiplying $M$ from the left with $f^i$ will extract the average history of the i-th stimulus (average $\\tilde{f}^i$). \n\nOn the other hand multiplying $M$ from the right with $f^i$ will extract the average future of the i-th stimulus, which we label as $p^i$: $$p_{\\tilde{\\tau}}=M_{\\tilde{\\tau}}f$$\nAnalogous to $\\tilde{f},p$ is as a 2-D matrix indexed by stimulus identity and $\\tilde{\\tau}$. Note also that instead of learning the average history we could have directly learned the average future via the successor representation."},
		{"id":"746249316c23c915","x":-1420,"y":10920,"width":879,"height":1560,"type":"text","text":"### Implementation of the JOR model\nTo model JOR we use the memory timeline stored in $\\tilde{f}$, as illustrated in the attached figure (section a.) We ran the model for 5000 trials with each trial consisting of 7 letters. We represented letters as delta pulses space by 100 time steps, followed by a probe composed of two randomly selected letters.\n\nParameters of the memory representation were as follows: k=8, the number of sequentially activated units for each letter was 100, the peak time of the first unit in the sequence was 50, and the peak time of the last unit was 2000.\n\nAfter the probes were presented, the model scanned sequentially (from more recent towards more distant past) the memory representation $\\tilde{f}$ and integrated the activation of units for each probe.\n\nOnce the threshold of 0.005 was reached for one of the probes, the scanning stopped and that probe was given as a response.\n\nThe model was run in a noise-free case and with added noise. In the latter case, the readout from $\\tilde{f}$ for each probe at every step of the scanning process was multiplied by a random amount of noise. The random amount was chosen from a uniform distribution with the lower limit of 0.15 and the upper limit of 1.85."},
		{"id":"aab351bcd68f2144","x":-340,"y":10671,"width":879,"height":1109,"type":"text","text":"### Implementation of the JOI model\n\nIn the JOI model, we presented a sequence of 12 letters 24 times in order to learn the associative representation stored in $M$ as illustrated in Figure 3b.\n\nParameters of the memory representation were teh same as in the JOR model. After the sequence was learned, 5000 probe trials were presented, each containing a stop position for the sequence and two randomly selected letters from the sequence were used as probes. \n\nThe probe letters had a lag between 2 and 7. The scanning procedure was analogous to JOR, except that instead of integrating the activation of units in the timeline of the past $\\tilde{f}$ we integrated the activation of the units in the timeline of the future $p$ (which is equivalent to integrating corresponding weights stored $M$). Similar to the JOR model, for JOI we also used a noise-free and noise case, with the same type of noise as in JOR."},
		{"id":"3e1116df28ca37b2","x":-340,"y":12110,"width":2464,"height":490,"color":"2","type":"text","text":"# Results\n\nWe quantified the results from the model in terms of RT and accuracy. We compared the results with the behavioral data from Tiganj et al. (2020). All of the results resembled those obtained in the behavioral versions of JOR and JOI. For correct trials, RT depended only on the lag to the more recent (JOR) / imminent (JOI) probe and not on the lag to the less recent/imminent probe as evidenced by the flat lines in Figure 4. This was a direct consequence of the fact that we used a serial self-terminating search type of model for both memory and prediction. Furthermore, for correct trials, the RT grew sublinearly with the lag. This was a result of the log-compression in the model timeline for both past and future. Finally, the accuracy generated by the model resembles the accuracy observed in the behavioral data. For the noise-free cases, the accuracy was 1 by construction."},
		{"id":"942e758ee0191d42","x":6220,"y":8800,"width":1120,"height":260,"type":"text","text":"# DeepSITH: Efficient Learning Via Decomposition of What and When Across Time Scales"}
	],
	"edges":[
		{"id":"9b103ebee8ace5f6","fromNode":"d71f31f1a5eed51b","fromSide":"right","toNode":"42f5f610dc6e711b","toSide":"left"},
		{"id":"89e23a8984f672ed","fromNode":"d71f31f1a5eed51b","fromSide":"right","toNode":"2dcb6b3f0f36712c","toSide":"left"},
		{"id":"65971287df75d222","fromNode":"42f5f610dc6e711b","fromSide":"right","toNode":"d93c3cf0e8ed3120","toSide":"left"},
		{"id":"01fa957e652da644","fromNode":"7052c4a20315b70a","fromSide":"right","toNode":"84448150afabe5e5","toSide":"left"},
		{"id":"f81154028508fbe7","fromNode":"84448150afabe5e5","fromSide":"left","toNode":"99dbd3ccbd5c6019","toSide":"right"},
		{"id":"8dd164dd6d86dbb2","fromNode":"94bf80281f6614c8","fromSide":"bottom","toNode":"1db6a665ed747c89","toSide":"top"},
		{"id":"f6b4244f82dc6d06","fromNode":"1db6a665ed747c89","fromSide":"bottom","toNode":"4ab2df8ec0d51b46","toSide":"top"},
		{"id":"91eadabb750ba430","fromNode":"4ab2df8ec0d51b46","fromSide":"bottom","toNode":"29c780d366d4de59","toSide":"top"},
		{"id":"4b25c3fad5ba2a51","fromNode":"2568279d551889ad","fromSide":"left","toNode":"84448150afabe5e5","toSide":"right"},
		{"id":"42adb9e8903aac2b","fromNode":"d0f83f8a8ddf1e00","fromSide":"right","toNode":"d73ecf2e61146783","toSide":"bottom","label":"The grey lines in Figure C depicts how F(s) and $\\tilde{f\\tilde{\\tau}}$ should behave in the time after a triggering stimulus different values of s and \\tilde{\\tau}"},
		{"id":"55c118777d3de187","fromNode":"8c7e315eb81d98d1","fromSide":"right","toNode":"6f3dd0faf5f6f93c","toSide":"bottom"},
		{"id":"df79f70398df274a","fromNode":"635819e439e07be2","fromSide":"bottom","toNode":"20f934262e924325","toSide":"top"},
		{"id":"98643f3e80480d63","fromNode":"65af1bc9abe7c765","fromSide":"bottom","toNode":"addc194e183a299d","toSide":"top"},
		{"id":"bd73617af8b588ce","fromNode":"addc194e183a299d","fromSide":"bottom","toNode":"d00aabd0902c45b2","toSide":"top"},
		{"id":"7fbe98bde20a550e","fromNode":"aefcde27384feab9","fromSide":"right","toNode":"31da639e525f5ad4","toSide":"top"},
		{"id":"cfd2a2ec8257c30f","fromNode":"aefcde27384feab9","fromSide":"left","toNode":"126d9f078cf4a837","toSide":"top"},
		{"id":"8f798af12fb738e7","fromNode":"126d9f078cf4a837","fromSide":"bottom","toNode":"76f0b78c0b018321","toSide":"top"},
		{"id":"788af02c99ab27a6","fromNode":"126d9f078cf4a837","fromSide":"bottom","toNode":"e211cea4fca9b4b3","toSide":"top"},
		{"id":"eebb6927a080de8a","fromNode":"aefcde27384feab9","fromSide":"bottom","toNode":"c3c1243bb6b8c6b8","toSide":"top"},
		{"id":"67ad6827ebd43ab3","fromNode":"e211cea4fca9b4b3","fromSide":"bottom","toNode":"746249316c23c915","toSide":"top"},
		{"id":"5a535d8cb257cf05","fromNode":"746249316c23c915","fromSide":"right","toNode":"164f2f61e0d17f2d","toSide":"bottom"},
		{"id":"5abb3d061a921e36","fromNode":"76f0b78c0b018321","fromSide":"bottom","toNode":"aab351bcd68f2144","toSide":"top"}
	]
}